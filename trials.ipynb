{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate \n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyMuPDFLoader,DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    \n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7313\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=model_name) \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "DB_FAISS_PATH = \"vectorstores/db_faiss\"\n",
    "db = FAISS.from_documents(text_chunks, embeddings)\n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINE TUNING T-5 SMALL MODEL ON CSV DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Context  \\\n",
      "0  I'm going through some things with my feelings...   \n",
      "1  I'm going through some things with my feelings...   \n",
      "2  I'm going through some things with my feelings...   \n",
      "3  I'm going through some things with my feelings...   \n",
      "4  I'm going through some things with my feelings...   \n",
      "5  I'm going through some things with my feelings...   \n",
      "6  I'm going through some things with my feelings...   \n",
      "7  I'm going through some things with my feelings...   \n",
      "8  I'm going through some things with my feelings...   \n",
      "9  I'm going through some things with my feelings...   \n",
      "\n",
      "                                            Response  \n",
      "0  If everyone thinks you're worthless, then mayb...  \n",
      "1  Hello, and thank you for your question and see...  \n",
      "2  First thing I'd suggest is getting the sleep y...  \n",
      "3  Therapy is essential for those that are feelin...  \n",
      "4  I first want to let you know that you are not ...  \n",
      "5  Heck, sure thing, hun!Feelings of 'depression'...  \n",
      "6  You are exhibiting some specific traits of a p...  \n",
      "7  That is intense. Depression is a liar. Sometim...  \n",
      "8  It sounds like you may be putting yourself las...  \n",
      "9  It must be really difficult to experience what...  \n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'intents_data/amod_mental_health_convo_train.csv'\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "csv_data.dropna(subset=['Context', 'Response'], inplace=True)\n",
    "print(csv_data.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "eval_dataset = dataset_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_column = \"Context\"\n",
    "target_column = \"Response\"\n",
    "model_name = \"google-t5/t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[input_column]\n",
    "    targets = examples[target_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ee67fad67e46b4844482c1cddf9ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5740b674208425b98d2393c3e3df2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example from tokenized_train_dataset:\")\n",
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example from tokenized_eval_dataset:\")\n",
    "print(tokenized_eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a1c28a712842ef985b4680dd918934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1755 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929afa7f3e944735b4ec7a642bff75a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.9701151847839355, 'eval_runtime': 279.922, 'eval_samples_per_second': 2.508, 'eval_steps_per_second': 0.314, 'epoch': 1.0}\n",
      "{'loss': 5.8242, 'grad_norm': 0.5609720945358276, 'learning_rate': 1.4301994301994305e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6cb80b568a44a1b20ae1b55b07ba27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.030130863189697, 'eval_runtime': 285.691, 'eval_samples_per_second': 2.457, 'eval_steps_per_second': 0.308, 'epoch': 2.0}\n",
      "{'loss': 4.2704, 'grad_norm': 2.343942880630493, 'learning_rate': 8.603988603988605e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be160abdaf39472290c64df5a61f1f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.593163013458252, 'eval_runtime': 283.0366, 'eval_samples_per_second': 2.48, 'eval_steps_per_second': 0.311, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e13516e71643e5812bf4d0a021fade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.519881010055542, 'eval_runtime': 180.9188, 'eval_samples_per_second': 3.88, 'eval_steps_per_second': 0.486, 'epoch': 4.0}\n",
      "{'loss': 3.9217, 'grad_norm': 0.6347969770431519, 'learning_rate': 2.9059829059829063e-06, 'epoch': 4.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb7a6479c9e48dc8498ef035e1b2835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5053179264068604, 'eval_runtime': 182.2619, 'eval_samples_per_second': 3.852, 'eval_steps_per_second': 0.483, 'epoch': 5.0}\n",
      "{'train_runtime': 11679.0112, 'train_samples_per_second': 1.201, 'train_steps_per_second': 0.15, 'train_loss': 4.558935407763533, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1755, training_loss=4.558935407763533, metrics={'train_runtime': 11679.0112, 'train_samples_per_second': 1.201, 'train_steps_per_second': 0.15, 'total_flos': 481066745856000.0, 'train_loss': 4.558935407763533, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/fine_tuned_t5_small\\\\tokenizer_config.json',\n",
       " './results/fine_tuned_t5_small\\\\special_tokens_map.json',\n",
       " './results/fine_tuned_t5_small\\\\spiece.model',\n",
       " './results/fine_tuned_t5_small\\\\added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./results/fine_tuned_t5_small')\n",
    "tokenizer.save_pretrained('./results/fine_tuned_t5_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "results_dir = './results/fine_tuned_t5_small'\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./results/fine_tuned_t5_small\\\\tokenizer_config.json',\n",
       " './results/fine_tuned_t5_small\\\\special_tokens_map.json',\n",
       " './results/fine_tuned_t5_small\\\\spiece.model',\n",
       " './results/fine_tuned_t5_small\\\\added_tokens.json',\n",
       " './results/fine_tuned_t5_small\\\\tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.save_pretrained(results_dir)\n",
    "tokenizer.save_pretrained(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = \"./results/fine_tuned_t5_small\"\n",
    "fine_tuned_model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "fine_tuned_tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def generate_response(input_text, max_length=50):\n",
    "    input_ids = fine_tuned_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output_ids = fine_tuned_model.generate(input_ids, max_length=max_length, do_sample=True)\n",
    "    response = fine_tuned_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "test_input = \"How can I overcome anxiety?\"\n",
    "generated_response = generate_response(test_input)\n",
    "print(generated_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: [{'generated_text': 'Was sollte ich tun, wenn ich mich ängst mache?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text2text-generation\", model='./results/fine_tuned_t5_small', tokenizer=tokenizer)\n",
    "\n",
    "# Test the model with a sample input\n",
    "response = generator(\"What should I do if I feel anxious?\")\n",
    "print(f\"Generated response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Use the following piece of code to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"You are a mental health assistant. Respond to the following query in a supportive and understanding manner:\\n\\nContext: {context}\\nResponse:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "model_path = \"./results/fine_tuned_t5_small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Are you a mental therapist or therapist? Please include questions with your name and your location below or click the button below to email address.\n"
     ]
    }
   ],
   "source": [
    "def generate_response_with_template(context, max_length=100):\n",
    "    prompt = template.format(context=context)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, do_sample=True)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test inference\n",
    "test_input = \"I think I need help\"\n",
    "generated_response = generate_response_with_template(test_input)\n",
    "print(generated_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm='./results/fine_tuned_t5_small', prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "results_dir = './openvino/fine_tuned_t5_small_openvino'\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/fine_tuned_t5_small\\\\tokenizer_config.json',\n",
       " './results/fine_tuned_t5_small\\\\special_tokens_map.json',\n",
       " './results/fine_tuned_t5_small\\\\spiece.model',\n",
       " './results/fine_tuned_t5_small\\\\added_tokens.json',\n",
       " './results/fine_tuned_t5_small\\\\tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small')\n",
    "tokenizer.save_pretrained('./results/fine_tuned_t5_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "ov_model = OVModelForSeq2SeqLM.from_pretrained('./results/fine_tuned_t5_small', export=True)\n",
    "tokenizer = T5TokenizerFast.from_pretrained('./results/fine_tuned_t5_small')\n",
    "tokenizer.save_pretrained('./results/fine_tuned_t5_small_openvino')\n",
    "ov_model.save_pretrained('./results/fine_tuned_t5_small_openvino')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the encoder to CPU ...\n",
      "Compiling the decoder to CPU ...\n",
      "Compiling the decoder to CPU ...\n"
     ]
    }
   ],
   "source": [
    "ov_model = OVModelForSeq2SeqLM.from_pretrained('./results/fine_tuned_t5_small_openvino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 7313\n",
      "Indices: [5842 5701 1279 4969 4824]\n",
      "Top results: [Document(page_content='and often wishes he could die so that\\nhe will not have to feel the loneliness,\\nemptiness, and the general feeling of\\nbeing down and hopeless. He says thathe does not have much to look forward', metadata={'source': 'data\\\\textbook.pdf', 'page': 309}), Document(page_content='progressive disability triggers mourning for the loss of function. The individual\\nexpresses feelings of grief and despair.', metadata={'source': 'data\\\\textbook.pdf', 'page': 302}), Document(page_content='mood, a reduction of negative feelings and negative self-concept, and an increase in\\nenergy and confidence. The euphoric mood can shift quite suddenly to sadness. Some¬', metadata={'source': 'data\\\\textbook.pdf', 'page': 74}), Document(page_content='Lorea, 2007; Roffman & Stephens, 2005).\\nFernandez-Montalvo et al. (2007) reported that coping with negative emotional', metadata={'source': 'data\\\\textbook.pdf', 'page': 263}), Document(page_content='itself in the face of the dysfunctionality.', metadata={'source': 'data\\\\textbook.pdf', 'page': 255})]\n"
     ]
    }
   ],
   "source": [
    "def search_faiss_all_texts(query, db, embeddings, k=5):\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    query_vector = np.array([query_vector])\n",
    "    D, I = db.index.search(query_vector, k)\n",
    "    print(f\"Distances: {D}\")\n",
    "    print(f\"Indices: {I}\")\n",
    "    return I[0]\n",
    "\n",
    "query = \"I am sad\"\n",
    "indices = search_faiss_all_texts(query, db, embeddings, k=5)\n",
    "print(f\"Number of texts: {len(text_chunks)}\")\n",
    "print(f\"Indices: {indices}\")\n",
    "results = [text_chunks[i] for i in indices]\n",
    "print(f\"Top results: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of code to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"./results/fine_tuned_t5_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])\n",
    "chain_type_kwargs={\"prompt\":PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\openvino_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_llm():\n",
    "    generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "    llm = HuggingFacePipeline(pipeline=generator)\n",
    "    return llm\n",
    "\n",
    "llm = load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    result = retrieval_chain({\"query\": question})\n",
    "    print(result)  \n",
    "    return result.get('answer', 'Answer not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'I am feeling anxious. What should I do?', 'result': '1 1 1 1 1 1'}\n",
      "Answer: Answer not found\n"
     ]
    }
   ],
   "source": [
    "question = \"I am feeling anxious. What should I do?\"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
